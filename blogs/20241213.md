---
layout: page
permalink: /blogs/20241213/index.html
title: "University rankings overlook academic retractions: A postplagiarism perspective"
---

# University rankings overlook academic retractions: A postplagiarism perspective

<br>
<div>
<img src="/blogs/20241213figure/Figure 6.png">
</div>
<br>

> Dec 13, 2024 — by Gengyan Tang.
> The original article was published on [Postplagiarism](https://postplagiarism.com/2024/11/07/artificial-intelligence-tools-may-widen-the-gap-between-international-students-from-different-language-backgrounds/).


Each year, when university rankings are released, many institutions proudly showcase their standings on their official websites—a routine display in higher education that rarely shifts. [Critics often argue that these rankings are essentially “paid badges,” failing to reflect a school’s true academic reputation and research quality](https://www.nytimes.com/2024/01/06/us/college-rankings-us-news.html).

However, in the [postplagiarism era](https://postplagiarism.com/2024/08/21/intro/), these criticisms need to expand to another overlooked issue: university rankings’ neglect of the rising number of retractions. [Our recent study sheds light on the misuse of AIGC in publications and related plagiarism concerns—issues that may lead to more retractions across academia](https://doi.org/10.3138/jsp-2023-0079).

These retractions highlight crucial problems in research integrity, quality assurance, and institutional accountability, precisely the concerns emphasized in the postplagiarism era. Yet, ranking systems have largely ignored these factors, continuing to emphasize metrics like publication counts and citation rates as measures of success. These skewed metrics are then presented to consumers—students and parents—as guidance for their educational investments.

## The Numbers Game: Flaws in University Ranking Systems

The basic mechanism of university rankings is a game of metrics. They design indicators, collect data, and then simplify it. For instance, research quality might be broken down into dimensions like paper count, citation counts, or average citations per researcher, each weighted in scoring the “research quality” metric.

Yet, as [past reports](https://retractionwatch.com/2023/06/08/did-a-nasty-publishing-scheme-help-an-indian-dental-school-win-high-rankings/) have shown, such ranking methods have serious flaws. Take the example of Saveetha Dental College in Chennai, India, which made headlines for reportedly pressuring undergraduates to publish papers, thereby climbing up in rankings.

In the end, these quantified standards often become little more than a numbers game, manipulated by savvy players. However, besides metric manipulation, another critical blind spot persists: actual research quality.

## The Big Retractions: Are Rankings Paying Attention?

Most university rankings heavily weigh “research quality” or “research strength,” often by counting papers published by a university’s researchers in high-impact journals or indexed databases.

But with major retractions due to data falsification, plagiarism, or other research misconduct, there’s no clarity on whether these rankings account for such papers. The four major ranking systems—QS, US News, THE, and ShanghaiRanking—do not specify how they handle retractions within their metrics.

<style>
.image-container {
  text-align: center;
  margin-bottom: 20px;
}

.image-container img {
  width: 500px;
  height: 400px;
  border: 2px solid #ccc;
  border-radius: 8px;
  padding: 5px;
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
}

.image-container p {
  margin-top: 10px;
  font-size: 14px;
  color: #555;
}
</style>

<div class="image-container">
  <img src="/blogs/20241213figure/Figure 1.png" alt="THE Ranking Indicators Explanation">
  <p>Figure 1. Explanation of THE’s Ranking Indicators</p>
</div>

<div class="image-container">
  <img src="/blogs/20241213figure/Figure 2.png" alt="US News Ranking Indicators Explanation">
  <p>Figure 2. Explanation of US News’s Ranking Indicators</p>
</div>

<div class="image-container">
  <img src="/blogs/20241213figure/Figure 3.png" alt="QS Ranking Indicators Explanation">
  <p>Figure 3. Explanation of QS’s Ranking Indicators</p>
</div>

<div class="image-container">
  <img src="/blogs/20241213figure/Figure 4.png" alt="Shanghai Ranking Indicators Explanation">
  <p>Figure 4. Explanation of Shanghai Ranking’s Ranking Indicators</p>
</div>

Another tricky issue lies in timing: a paper might be published, counted, and then later retracted. Should rankings adjust their scores retroactively?

Such issues expose potential flaws in ranking methodologies, particularly regarding research integrity.

## A Possible Solution

From both a principled and practical perspective, I can suggest some potential solutions:

At the principled level, the postplagiarism era underscores the importance of responsibility. Dr. Eaton notes, [“Humans, not technology, are held responsible for the accuracy, validity, reliability, and trustworthiness of scientific and scholarly outputs.”](https://edintegrity.biomedcentral.com/articles/10.1007/s40979-023-00144-1) This principle should apply to the way university rankings formulate their metrics:

- **Accuracy**  
  University rankings must accurately account for retracted papers in their metrics. Only by doing so can they ensure fairness to institutions that are committed to maintaining a culture of research integrity.

- **Validity**  
  When university rankings tally publication counts, citation rates, and other research metrics, they must ensure that only valid papers are included. Articles retracted for plagiarism or falsification should be excluded from these counts.

- **Reliability**  
  University rankings should also consider the reliability of the research output they measure. Papers produced with an over-reliance on generative AI tools, resulting in substandard quality, should not contribute to an institution’s research score.

- **Trustworthiness**  
  University rankings need to demonstrate their commitment to research integrity in the postplagiarism era. By emphasizing this, they can establish themselves as trustworthy evaluators in higher education.

Furthermore, in the post-plagiarism era, Dr. Eaton suggests that traditional definitions of plagiarism may no longer apply. This perspective can also be extended to how we understand university rankings. In the post-plagiarism era, the definition of university rankings should incorporate the core values of research and academic integrity. This shift is necessary to ensure that ranking systems reflect not only the academic quality and reputation of an institution but also its commitment to research and academic integrity—providing parents and students with a more comprehensive basis for evaluation.

At the practical level, one solution for rankings could be to incorporate an “research integrity metric” or to adjust an institution’s research quality score based on its number of retractions. This could be framed as a penalty mechanism, for example, by subtracting retracted papers from the institution’s total publications for that year.

In short, university rankings must take research integrity into account, especially when assessing research strength or quality. Only by doing so can they fairly represent institutions committed to genuine, ethical research.

> About the author: Gengyan Tang, MA, is a PhD student in the Werklund School of Education at the University of Calgary. His research interests include research integrity and academic integrity.

<br>

## Leave a Message 欢迎留言

<br>

{% include disqus.html %} 

<br>
